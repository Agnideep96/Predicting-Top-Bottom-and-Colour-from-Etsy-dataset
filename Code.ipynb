{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df=pd.read_parquet('C:/Users/91847/Downloads/train', engine=\"pyarrow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('etsy_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Number of rows in train: {len(df):,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Number of unique products in train: {len(df['product_id'].unique()):,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Number of columns in train: {len(df.columns):,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Number of unique top category in train: {len(df['top_category_id'].unique()):,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Number of unique bottom category in train: {len(df['bottom_category_id'].unique()):,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Number of colour category in train: {len(df['color_id'].unique()):,}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEALING WITH NULL VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking NULL from the title column\n",
    "null_values = df['title'].isnull().sum()\n",
    "print('Number of null values in the Age column:', null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing NULL from the title\n",
    "df = df.dropna(subset=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking NULL from the descriotion column\n",
    "null_values = df['description'].isnull().sum()\n",
    "print('Number of null values in the Description column:', null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking NULL from the descriotion column\n",
    "null_values = df['tags'].isnull().sum()\n",
    "print('Number of null values in the Tags column:', null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing NULL from the tags\n",
    "df = df.dropna(subset=['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking top_category_text/id from the descriotion column\n",
    "null_values = df[[0]].isnull().sum()\n",
    "print('Number of null values in the Tags column:', null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['color_id']] = df[['color_id']].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Cleaned Data\n",
    "\n",
    "# df.to_csv('etsy_train_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_bci = df['top_category_id'].unique()\n",
    "print (len(unique_bci))\n",
    "type(unique_bci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the important columns only to predict top category id\n",
    "df2_top = df.iloc[: , [1, 2, 3, 17]].copy()\n",
    "df2_top.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Selecting the important columns only to predict bottom category id\n",
    "df2_bottom=pd.read_csv('C:/Users/91847/Desktop/DCU/Assignments/Etsy ML/etsy_train_clean1.csv')\n",
    "df2_bottom = df2_bottom.iloc[: , [1, 2, 3, 4, 18, 16]].copy()\n",
    "df2_bottom.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the title description and tags into a single column\n",
    "#Top Category Prediction\n",
    "df2_top['text'] = df2_top['title'] + ' ' + df2_top['description'] + ' ' + df2_top['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the title description and tags into a single column\n",
    "#Bottom Category Prediction\n",
    "df2_bottom['text'] = df2_bottom['title'] + ' ' + df2_bottom['description'] + ' ' + df2_bottom['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_bottom.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-arranging the columns \n",
    "df3_top=df2_top[['text', 'top_category_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-arranging the columns \n",
    "df3_bottom=df2_bottom[['product_id','text', 'top_category_id', 'bottom_category_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_bottom.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Reading the data into CSV for Top category Prediction\n",
    "df3_top=pd.read_csv('C:/Users/91847/Desktop/DCU/Assignments/Etsy ML/etsy_train_top_catagory_prediction1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the data into CSV for Bottom category Prediction\n",
    "df3_bottom.to_csv('C:/Users/91847/Desktop/DCU/Assignments/Etsy ML/etsy_train_bottom_catagory_prediction3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df3_bottom=pd.read_csv('etsy_train_bottom_catagory_prediction3.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_bottom.shape[0]\n",
    "#df3_bottom.head(10)\n",
    "\n",
    "#df3_top.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Top Category\n",
    "\n",
    "# corpus = []\n",
    "# for i in range(0, 210564):  \n",
    "#   text = re.sub('[^a-zA-Z]', ' ', df3_top['text'][i])\n",
    "#   text = text.lower()\n",
    "#   text = text.split()\n",
    "#   ps = PorterStemmer()\n",
    "#   all_stopwords = stopwords.words('english')\n",
    "#   all_stopwords.remove('not')\n",
    "#   text = [ps.stem(word) for word in text if not word in set(all_stopwords)]\n",
    "#   text = ' '.join(text)\n",
    "#   corpus.append(text)\n",
    "\n",
    "#For Bottom Category\n",
    "\n",
    "corpus = []\n",
    "for i in range(0, 210570):  \n",
    "  text = re.sub('[^a-zA-Z]', ' ', df3_bottom['text'][i])\n",
    "  text = text.lower()\n",
    "  text = text.split()\n",
    "  ps = PorterStemmer()\n",
    "  all_stopwords = stopwords.words('english')\n",
    "  all_stopwords.remove('not')\n",
    "  text = [ps.stem(word) for word in text if not word in set(all_stopwords)]\n",
    "  text = ' '.join(text)\n",
    "  corpus.append(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 600) #max_features = 'n' to reduce the feature matrix\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "#y = df3_top.iloc[:, 2].values\n",
    "y = df3_bottom.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=df3_bottom.iloc[:, 3].values\n",
    "product_id=df3_bottom.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding topid to feature matrix\n",
    "import numpy as np\n",
    "X = np.column_stack((X,z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OverSampling (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X_over, y_over = SMOTE().fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking balance of data\n",
    "from collections import Counter\n",
    "Counter(y_over)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size = 0.20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Random Forest model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #FOR Naive Bayes (bas_model)\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# classifier = GaussianNB()\n",
    "# classifier.fit(X_train, y_train)\n",
    "\n",
    "#Random Forest Classification (Top_category_prediction)\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# classifier = RandomForestClassifier(n_estimators = 50, criterion = 'entropy')\n",
    "# classifier.fit(X_train, y_train)\n",
    "\n",
    "#K-Nearest Neighbour (Bottom_Category_Prediction)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 500, weights='distance', metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "#cm = confusion_matrix(y_test, y_pred)\n",
    "#print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colour Prediction(Performed separately at kaagle notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow tensorflow-gpu opencv-python matplotlib\n",
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Memory Consumption Growth to block out of memory error\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the GPU detection\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing images from different extensions\n",
    "data_dir = 'data'\n",
    "image_exts = ['jpeg','jpg', 'bmp', 'png']\n",
    "data_dir=\"/kaggle/input/etsy-colour/Etsy/data/train\"\n",
    "for image_class in os.listdir(data_dir): \n",
    "    for image in os.listdir(os.path.join(data_dir, image_class)):\n",
    "        image_path = os.path.join(data_dir, image_class, image)\n",
    "        try: \n",
    "            img = cv2.imread(image_path)\n",
    "            tip = imghdr.what(image_path)\n",
    "            if tip not in image_exts: \n",
    "                print('Image not in ext list {}'.format(image_path))\n",
    "                os.remove(image_path)\n",
    "        except Exception as e: \n",
    "            print('Issue with image {}'.format(image_path))\n",
    "            # os.remove(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = tf.keras.utils.image_dataset_from_directory('/kaggle/input/etsy-colour/Etsy/data/train')\n",
    "data_iterator = data.as_numpy_iterator()\n",
    "data_iterator = data.as_numpy_iterator()\n",
    "\n",
    "#checking 6 images\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(15,15))\n",
    "for idx, img in enumerate(batch[0][:6]):\n",
    "    ax[idx].imshow(img.astype(int))\n",
    "    ax[idx].title.set_text(batch[1][idx])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "data = data.map(lambda x,y: (x/255, y))\n",
    "data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into train-test & validation\n",
    "train_size = int(len(data)*.7)\n",
    "val_size = int(len(data)*.2)+1\n",
    "test_size = int(len(data)*.1)+1\n",
    "\n",
    "train = data.take(train_size)\n",
    "val = data.skip(train_size).take(val_size)\n",
    "test = data.skip(train_size+val_size).take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input\n",
    "# Define the ResNet50 model\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(256, 256, 3)\n",
    ")\n",
    "\n",
    "# Freeze all the layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Conv2D,MaxPool2D,GlobalAveragePooling2D,AveragePooling2D\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(6, activation='softmax')(x)\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "model=Model(inputs=base_model.input,outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir='logs'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "#hist = model.fit(train_generator, epochs=20, validation_data=val, callbacks=[tensorboard_callback])\n",
    "hist = model.fit(train, epochs=20, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the performance \n",
    "fig = plt.figure()\n",
    "plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdb921f148556a6d6dac921fbfe03bf8d4a28fe66c88ba8634a7dfbff0f8fbe0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
